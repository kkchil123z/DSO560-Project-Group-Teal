{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# \"Fit\", \"Dry Clean Only\",\"Category\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook contains the process to select best model for \"Fit\",\"Dry Clean Only\" and \"Category\". There are 4 steps involved in the model building process which is EDA(exploratory data analysis), Text preprocessing, Model building, and 10-Fold cross validation. In the Final section, we deploy the best model in each of the 3 categories on 1000 row of the full_data to see the sample output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.options.mode.chained_assignment = None\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk import punkt\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import accuracy_score,recall_score,f1_score\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.ensemble import RandomForestClassifier,GradientBoostingClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Funciton"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "lem = WordNetLemmatizer()\n",
    "\n",
    "def lem_sentences(sentence):\n",
    "    tokens = nltk.word_tokenize(sentence)\n",
    "    lemmed_tokens = [lem.lemmatize(token) for token in tokens]\n",
    "    return ' '.join(lemmed_tokens)\n",
    "\n",
    "#I copy this function from this stackoverflow website\n",
    "# https://stackoverflow.com/questions/43795310/apply-porters-stemmer-to-a-pandas-column-for-each-word\n",
    "\n",
    "def cleanHtml(sentence):\n",
    "    cleanr = re.compile('<.*?>')\n",
    "    cleantext = re.sub(cleanr, ' ', str(sentence))\n",
    "    return cleantext\n",
    "\n",
    "def keepAlpha(sentence):\n",
    "    alpha_sent = \"\"\n",
    "    for word in word_tokenize(sentence):\n",
    "        alpha_word = re.sub('[^\\w]+', '', word)\n",
    "        alpha_sent += alpha_word\n",
    "        alpha_sent += \" \"\n",
    "    alpha_sent = alpha_sent.strip()\n",
    "    return alpha_sent\n",
    "\n",
    "def cleanPunc(sentence): #function to clean the word of any punctuation or special characters\n",
    "    cleaned = re.sub(r'[?|!|\\'|\"|#]',r'',sentence)\n",
    "    cleaned = re.sub(r'[,|)|(|\\|/]',r' ',cleaned)\n",
    "    cleaned = cleaned.strip()\n",
    "    cleaned = cleaned.replace(\"\\n\",\" \")\n",
    "    \n",
    "    return cleaned\n",
    "#The above 4 define function are based on this github page\n",
    "#https://github.com/nkartik94/Multi-Label-Text-Classification/blob/master/Mark_6.ipynb\n",
    "           \n",
    "\n",
    "def cleanComma(sentence): #function to clean the word of any punctuation or special characters\n",
    "    cleaned = sentence.strip(',')\n",
    "    cleaned = re.sub(r',{2,}',r',',cleaned)\n",
    "    return cleaned\n",
    "\n",
    "cloth_list=['top','bottom','one piece','sweater','blazers_coats_jackets','sweatshirt_hoodie']\n",
    "non_cloth_list=['shoe','accessory']\n",
    "\n",
    "def is_clothing(x):\n",
    "    #create target value to determine if item is clothing\n",
    "    if x in cloth_list:\n",
    "        return 1\n",
    "    elif x in non_cloth_list:\n",
    "        return 0\n",
    "\n",
    "top_list=['top','sweater','blazers_coats_jackets','sweatshirt_hoodie']\n",
    "\n",
    "def is_top(x):\n",
    "    #group the top clothing into the same tag\n",
    "    #create target value for \"category\" tag\n",
    "    if x in top_list:\n",
    "        return 'top'\n",
    "    else:\n",
    "        return x    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tag_data = pd.read_csv('tagged_product_attributes.csv')\n",
    "full_data = pd.read_csv('full_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Align the attribute_name and #attribute_value\n",
    "tag_data['attribute_value']=tag_data['attribute_value'].str.lower()\n",
    "tag_data['attribute_name']=tag_data['attribute_name'].str.lower()\n",
    "\n",
    "tag_data.loc[tag_data['attribute_name']=='drycleanonly','attribute_name']='dry_clean_only'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#the additional 3 category our group chose\n",
    "fit_tag = tag_data[tag_data['attribute_name']=='fit']\n",
    "dryclean_tag = tag_data[tag_data['attribute_name']=='dry_clean_only']\n",
    "\n",
    "#this dataframe would also be used to build model to determine if the item is clothing\n",
    "category_tag=tag_data[tag_data['attribute_name']=='category']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Build Model to determine if the item is clothing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The category \"Fit\" and \"Dry Clean Only\" is for clothing item only, so we should build a model to determine whether an item is clothing or not.\n",
    "\n",
    "I used the data with \"attribute_name\" of \"category\" to generate input for the model. If the item has \"shoe\" or \"accessory\" value, it belong to the \"non-clothing\" category"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#aligm attribute value\n",
    "#this step help us to drop duplicate value later\n",
    "category_tag.loc[category_tag['attribute_value']=='blazers, coats & jackets','attribute_value']='blazers_coats_jackets'\n",
    "category_tag.loc[category_tag['attribute_value']=='blazerscoatsjackets','attribute_value']='blazers_coats_jackets'\n",
    "category_tag.loc[category_tag['attribute_value']=='onepiece','attribute_value']='one piece'\n",
    "category_tag.loc[category_tag['attribute_value']=='sweatshirthoodie','attribute_value']='sweatshirt_hoodie'\n",
    "category_tag.loc[category_tag['attribute_value']=='sweatshirt & hoodie','attribute_value']='sweatshirt_hoodie'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "top                      1524\n",
       "shoe                     1460\n",
       "bottom                   1365\n",
       "accessory                 640\n",
       "one piece                 605\n",
       "sweater                   545\n",
       "blazers_coats_jackets     433\n",
       "sweatshirt_hoodie         208\n",
       "Name: attribute_value, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#inspect the distribution of different category\n",
    "category_tag['attribute_value'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#apply funcation to get the target the model\n",
    "category_tag['clothing']=category_tag['attribute_value'].apply(is_clothing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    4680\n",
       "0    2100\n",
       "Name: clothing, dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#inspect the distribution of target\n",
    "category_tag['clothing'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>product_id</th>\n",
       "      <th>product_color_id</th>\n",
       "      <th>attribute_name</th>\n",
       "      <th>attribute_value</th>\n",
       "      <th>file</th>\n",
       "      <th>clothing</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>77339</th>\n",
       "      <td>01DPKMXBJEMQWQWAZHGJK1RS8Z</td>\n",
       "      <td>01DPKMXBJJGRSZAPCX97AXGSN8</td>\n",
       "      <td>category</td>\n",
       "      <td>top</td>\n",
       "      <td>additional</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113233</th>\n",
       "      <td>01DPKMXBJEMQWQWAZHGJK1RS8Z</td>\n",
       "      <td>01DPKMXBJJGRSZAPCX97AXGSN8</td>\n",
       "      <td>category</td>\n",
       "      <td>bottom</td>\n",
       "      <td>additional</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3873</th>\n",
       "      <td>01DSP2PPDS647HVVN1GAA1F68Q</td>\n",
       "      <td>01DSP2PPE1RBXCBN4D0HFJCH49</td>\n",
       "      <td>category</td>\n",
       "      <td>top</td>\n",
       "      <td>initial_tags</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82153</th>\n",
       "      <td>01DSP2PPDS647HVVN1GAA1F68Q</td>\n",
       "      <td>01DSP2PPE1RBXCBN4D0HFJCH49</td>\n",
       "      <td>category</td>\n",
       "      <td>accessory</td>\n",
       "      <td>additional</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71321</th>\n",
       "      <td>01DT0DJH0G3NX1VGEQW3GFH4CG</td>\n",
       "      <td>01DT0DJH28GVQFR1AQAKK7GFKB</td>\n",
       "      <td>category</td>\n",
       "      <td>top</td>\n",
       "      <td>additional</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82293</th>\n",
       "      <td>01DT0DJH0G3NX1VGEQW3GFH4CG</td>\n",
       "      <td>01DT0DJH28GVQFR1AQAKK7GFKB</td>\n",
       "      <td>category</td>\n",
       "      <td>blazers_coats_jackets</td>\n",
       "      <td>additional</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19497</th>\n",
       "      <td>01DT0DJW9KH3TSSDJ4V5HB23KM</td>\n",
       "      <td>01DT0DJW9VP9W3W4W8N4TNP9H8</td>\n",
       "      <td>category</td>\n",
       "      <td>blazers_coats_jackets</td>\n",
       "      <td>initial_tags</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82227</th>\n",
       "      <td>01DT0DJW9KH3TSSDJ4V5HB23KM</td>\n",
       "      <td>01DT0DJW9VP9W3W4W8N4TNP9H8</td>\n",
       "      <td>category</td>\n",
       "      <td>sweater</td>\n",
       "      <td>additional</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8601</th>\n",
       "      <td>01DT514ZTH8FXVKG9J2GXYT8A9</td>\n",
       "      <td>01DT514ZTQZMW9H7AMH8ATAZM8</td>\n",
       "      <td>category</td>\n",
       "      <td>top</td>\n",
       "      <td>initial_tags</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75365</th>\n",
       "      <td>01DT514ZTH8FXVKG9J2GXYT8A9</td>\n",
       "      <td>01DT514ZTQZMW9H7AMH8ATAZM8</td>\n",
       "      <td>category</td>\n",
       "      <td>sweatshirt_hoodie</td>\n",
       "      <td>additional</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8323</th>\n",
       "      <td>01DT515KEMTBG2ZTNRHYQ29PCT</td>\n",
       "      <td>01DT515KET8PG0ATT955Q3N0VX</td>\n",
       "      <td>category</td>\n",
       "      <td>accessory</td>\n",
       "      <td>initial_tags</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75400</th>\n",
       "      <td>01DT515KEMTBG2ZTNRHYQ29PCT</td>\n",
       "      <td>01DT515KET8PG0ATT955Q3N0VX</td>\n",
       "      <td>category</td>\n",
       "      <td>top</td>\n",
       "      <td>additional</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1683</th>\n",
       "      <td>01DTJAZ7F9DRFPNF8W1H7SFT20</td>\n",
       "      <td>01DTJAZ7FJVFXV2QWZVSZ4RNMY</td>\n",
       "      <td>category</td>\n",
       "      <td>top</td>\n",
       "      <td>initial_tags</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98437</th>\n",
       "      <td>01DTJAZ7F9DRFPNF8W1H7SFT20</td>\n",
       "      <td>01DTJAZ7FJVFXV2QWZVSZ4RNMY</td>\n",
       "      <td>category</td>\n",
       "      <td>accessory</td>\n",
       "      <td>additional</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        product_id            product_color_id attribute_name  \\\n",
       "77339   01DPKMXBJEMQWQWAZHGJK1RS8Z  01DPKMXBJJGRSZAPCX97AXGSN8       category   \n",
       "113233  01DPKMXBJEMQWQWAZHGJK1RS8Z  01DPKMXBJJGRSZAPCX97AXGSN8       category   \n",
       "3873    01DSP2PPDS647HVVN1GAA1F68Q  01DSP2PPE1RBXCBN4D0HFJCH49       category   \n",
       "82153   01DSP2PPDS647HVVN1GAA1F68Q  01DSP2PPE1RBXCBN4D0HFJCH49       category   \n",
       "71321   01DT0DJH0G3NX1VGEQW3GFH4CG  01DT0DJH28GVQFR1AQAKK7GFKB       category   \n",
       "82293   01DT0DJH0G3NX1VGEQW3GFH4CG  01DT0DJH28GVQFR1AQAKK7GFKB       category   \n",
       "19497   01DT0DJW9KH3TSSDJ4V5HB23KM  01DT0DJW9VP9W3W4W8N4TNP9H8       category   \n",
       "82227   01DT0DJW9KH3TSSDJ4V5HB23KM  01DT0DJW9VP9W3W4W8N4TNP9H8       category   \n",
       "8601    01DT514ZTH8FXVKG9J2GXYT8A9  01DT514ZTQZMW9H7AMH8ATAZM8       category   \n",
       "75365   01DT514ZTH8FXVKG9J2GXYT8A9  01DT514ZTQZMW9H7AMH8ATAZM8       category   \n",
       "8323    01DT515KEMTBG2ZTNRHYQ29PCT  01DT515KET8PG0ATT955Q3N0VX       category   \n",
       "75400   01DT515KEMTBG2ZTNRHYQ29PCT  01DT515KET8PG0ATT955Q3N0VX       category   \n",
       "1683    01DTJAZ7F9DRFPNF8W1H7SFT20  01DTJAZ7FJVFXV2QWZVSZ4RNMY       category   \n",
       "98437   01DTJAZ7F9DRFPNF8W1H7SFT20  01DTJAZ7FJVFXV2QWZVSZ4RNMY       category   \n",
       "\n",
       "              attribute_value          file  clothing  \n",
       "77339                     top    additional         1  \n",
       "113233                 bottom    additional         1  \n",
       "3873                      top  initial_tags         1  \n",
       "82153               accessory    additional         0  \n",
       "71321                     top    additional         1  \n",
       "82293   blazers_coats_jackets    additional         1  \n",
       "19497   blazers_coats_jackets  initial_tags         1  \n",
       "82227                 sweater    additional         1  \n",
       "8601                      top  initial_tags         1  \n",
       "75365       sweatshirt_hoodie    additional         1  \n",
       "8323                accessory  initial_tags         0  \n",
       "75400                     top    additional         1  \n",
       "1683                      top  initial_tags         1  \n",
       "98437               accessory    additional         0  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#There are 7 products which has multiple tag under 'category'\n",
    "#remove those record\n",
    "tdf=category_tag.drop_duplicates(subset=['product_id','attribute_value'])\n",
    "tdf2=category_tag.drop_duplicates(subset='product_id')\n",
    "\n",
    "#list of product that have multiple category tag\n",
    "tlist=tdf[~(tdf.index.isin(tdf2.index))]['product_id'].values\n",
    "tdf[tdf['product_id'].isin(tlist)].sort_values('product_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Exclude the record\n",
    "category_tag2=category_tag[~(category_tag['product_id'].isin(tlist))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Keep relevant column and remove duplicate record based on \"product_id\" and \"attribute_value\"\n",
    "category_tagm=category_tag2[['product_id','attribute_name','attribute_value','clothing']].\\\n",
    "drop_duplicates(subset=['product_id','attribute_value'])\n",
    "\n",
    "#remove duplicate record based on product id\n",
    "full_datam=full_data[['product_id','brand','brand_category','product_full_name','description','details']]\\\n",
    ".drop_duplicates(subset=['product_id'])\n",
    "category_all=category_tagm.merge(full_datam,how='left',on='product_id')\n",
    "#Fill Null value\n",
    "category_all=category_all.fillna('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Concatnate string as input\n",
    "category_all['input'] =category_all[['brand','brand_category','product_full_name','description','details']]\\\n",
    ".agg(' '.join, axis=1).str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    2980\n",
       "0     982\n",
       "Name: clothing, dtype: int64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Keep the relevant column and inspect the distribution of target value\n",
    "category_df=category_all[['product_id','clothing','input']]\n",
    "category_df['clothing'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "### 1.2 Text Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "category_df['input']= category_df['input'].apply(lem_sentences)\n",
    "category_df['input'] = category_df['input'].apply(cleanHtml)\n",
    "category_df['input'] = category_df['input'].apply(cleanPunc)\n",
    "category_df['input'] = category_df['input'].apply(keepAlpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save the dataframe for furture model training\n",
    "category_df.to_csv('is_clothing_or_not.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Model Building"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we want to predict if a item is clothing or not, 1 represent it is an clothing item and 0 represent not, so we build several binary classification model to determine which is the best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#spliting data for training and testing\n",
    "X=category_df['input']\n",
    "y=category_df['clothing']\n",
    "X_train, X_test,y_train, y_test = train_test_split(X,y, random_state=42, test_size=0.30, shuffle=True,stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import stopword\n",
    "stop_list=stopwords.words('english')\n",
    "\n",
    "#tf-idf vectorize\n",
    "#this vecotirzer is used in every model in this notebook\n",
    "vectorizer = TfidfVectorizer(ngram_range=(1,3),\n",
    "                             token_pattern=r'\\b[a-zA-Z0-9]{3,}\\b',\n",
    "                             max_df=0.5,\n",
    "                             min_df=10, stop_words=stop_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy for Logistic Regression is      0.9957947855340622\n",
      "Test accuracy for Support Vector Classifier is 0.9991589571068125\n",
      "Test accuracy for Random Forest Classifier is 0.9932716568544996\n",
      "Test accuracy for kNearstNeibor is            0.9983179142136249\n",
      "Test accuracy for Gradient Boosted is         0.9831791421362489\n",
      "\n",
      "\n",
      "Test f1_score for Logistic Regression is      0.9972113775794758\n",
      "Test f1_score for Support Vector Classifier is 0.9994404029099049\n",
      "Test f1_score for Random Forest Classifier is 0.9955307262569834\n",
      "Test f1_score for kNearstNeibor is            0.9988814317673378\n",
      "Test f1_score for Gradient Boosted is         0.9889012208657049\n",
      "\n",
      "\n",
      "Test mix score for Logistic Regression is      0.996503081556769\n",
      "Test mix score for Support Vector Classifier is 0.9992996800083587\n",
      "Test mix score for Random Forest Classifier is 0.9944011915557415\n",
      "Test mix score for kNearstNeibor is            0.9985996729904814\n",
      "Test mix score for Gradient Boosted is         0.9860401815009769\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Using pipeline for applying logistic regression, svc,randon forest classifier, kNN, and gradient boosted\n",
    "#Using both accuracy and f1 score to select model\n",
    "LogReg_pipeline = Pipeline([('tfidf', vectorizer),\n",
    "                ('clf', LogisticRegression(solver='sag')),\n",
    "            ])\n",
    "\n",
    "SVC_pipeline = Pipeline([('tfidf', vectorizer),\n",
    "                ('clf', LinearSVC()),\n",
    "            ])\n",
    "\n",
    "RandomFC_pipeline = Pipeline([('tfidf', vectorizer),\n",
    "                ('clf', RandomForestClassifier()),\n",
    "            ])\n",
    "\n",
    "kNN_pipeline = Pipeline([('tfidf', vectorizer),\n",
    "                ('clf', KNeighborsClassifier()),\n",
    "            ])\n",
    "\n",
    "GradientFC_pipeline = Pipeline([('tfidf', vectorizer),\n",
    "                ('clf', GradientBoostingClassifier()),\n",
    "            ])\n",
    "\n",
    "\n",
    "LogReg_pipeline.fit(X_train, y_train)\n",
    "SVC_pipeline.fit(X_train, y_train)\n",
    "RandomFC_pipeline.fit(X_train, y_train)\n",
    "kNN_pipeline.fit(X_train, y_train)\n",
    "GradientFC_pipeline.fit(X_train, y_train)\n",
    "    \n",
    "    # calculating test accuracy\n",
    "prediction1 = LogReg_pipeline.predict(X_test)\n",
    "prediction2 = SVC_pipeline.predict(X_test)\n",
    "prediction3 = RandomFC_pipeline.predict(X_test)\n",
    "prediction4 = kNN_pipeline.predict(X_test)\n",
    "prediction5 = GradientFC_pipeline.predict(X_test)\n",
    "    \n",
    "accuracy1=accuracy_score(y_test, prediction1)\n",
    "accuracy2=accuracy_score(y_test, prediction2)\n",
    "accuracy3=accuracy_score(y_test, prediction3)\n",
    "accuracy4=accuracy_score(y_test, prediction4)\n",
    "accuracy5=accuracy_score(y_test, prediction5)\n",
    "    \n",
    "f1_score1=f1_score(y_test, prediction1)\n",
    "f1_score2=f1_score(y_test, prediction2)\n",
    "f1_score3=f1_score(y_test, prediction3)\n",
    "f1_score4=f1_score(y_test, prediction4)\n",
    "f1_score5=f1_score(y_test, prediction5)\n",
    "\n",
    "\n",
    "print('Test accuracy for Logistic Regression is      {}'.format(accuracy1))\n",
    "print('Test accuracy for Support Vector Classifier is {}'.format(accuracy2))\n",
    "print('Test accuracy for Random Forest Classifier is {}'.format(accuracy3))\n",
    "print('Test accuracy for kNearstNeibor is            {}'.format(accuracy4))\n",
    "print('Test accuracy for Gradient Boosted is         {}'.format(accuracy5))\n",
    "print(\"\\n\")\n",
    "print('Test f1_score for Logistic Regression is      {}'.format(f1_score1))\n",
    "print('Test f1_score for Support Vector Classifier is {}'.format(f1_score2))\n",
    "print('Test f1_score for Random Forest Classifier is {}'.format(f1_score3))\n",
    "print('Test f1_score for kNearstNeibor is            {}'.format(f1_score4))\n",
    "print('Test f1_score for Gradient Boosted is         {}'.format(f1_score5))\n",
    "print(\"\\n\")\n",
    "print('Test mix score for Logistic Regression is      {}'.format((accuracy1+f1_score1)/2))\n",
    "print('Test mix score for Support Vector Classifier is {}'.format((accuracy2+f1_score2)/2))\n",
    "print('Test mix score for Random Forest Classifier is {}'.format((accuracy3+f1_score3)/2))\n",
    "print('Test mix score for kNearstNeibor is            {}'.format((accuracy4+f1_score4)/2))\n",
    "print('Test mix score for Gradient Boosted is         {}'.format((accuracy5+f1_score5)/2))\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **Support Vector Classifier** model seems to perform the best, but we should cross validate to see if the model performance is consistent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 10-Fold Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test mix score based on 10-fold CV for Logistic Regression is      0.9962275438945213\n",
      "Test mix score based on 10-fold CV for Support Vector Classifier is 0.9989508805253144\n",
      "Test mix score based on 10-fold CV for Random Forest Classifier is 0.995133880947075\n",
      "Test mix score based on 10-fold CV for kNearstNeibor is            0.9955809644473869\n",
      "Test mix score based on 10-fold CV for Gradient Boosted is         0.9869010309289223\n"
     ]
    }
   ],
   "source": [
    "LogReg_pipeline = Pipeline([('tfidf', vectorizer),\n",
    "                ('clf', LogisticRegression(solver='sag')),])\n",
    "\n",
    "SVC_pipeline = Pipeline([('tfidf', vectorizer),\n",
    "                ('clf', LinearSVC()),\n",
    "            ])\n",
    "\n",
    "RandomFC_pipeline = Pipeline([('tfidf', vectorizer),\n",
    "                ('clf', RandomForestClassifier()),\n",
    "            ])\n",
    "\n",
    "kNN_pipeline = Pipeline([('tfidf', vectorizer),\n",
    "                ('clf', KNeighborsClassifier()),\n",
    "            ])\n",
    "\n",
    "GradientFC_pipeline = Pipeline([('tfidf', vectorizer),\n",
    "                ('clf', GradientBoostingClassifier()),\n",
    "            ])\n",
    "\n",
    "X=category_df['input']\n",
    "y=category_df['clothing']\n",
    "\n",
    "cv_score1 =cross_val_score(LogReg_pipeline, X, y, scoring = 'accuracy',cv = 10)\n",
    "cv_score2 =cross_val_score(SVC_pipeline, X, y, scoring = 'accuracy',cv = 10)\n",
    "cv_score3 =cross_val_score(RandomFC_pipeline, X, y, scoring = 'accuracy',cv = 10)\n",
    "cv_score4 =cross_val_score(kNN_pipeline, X, y, scoring = 'accuracy',cv = 10)\n",
    "cv_score5 =cross_val_score(GradientFC_pipeline, X, y, scoring = 'accuracy',cv = 10)\n",
    "\n",
    "cv_score6 =cross_val_score(LogReg_pipeline, X, y, scoring = 'f1',cv = 10)\n",
    "cv_score7 =cross_val_score(SVC_pipeline, X, y, scoring = 'f1',cv = 10)\n",
    "cv_score8 =cross_val_score(RandomFC_pipeline, X, y, scoring = 'f1',cv = 10)\n",
    "cv_score9 =cross_val_score(kNN_pipeline, X, y, scoring = 'f1',cv = 10)\n",
    "cv_score10 =cross_val_score(GradientFC_pipeline, X, y, scoring = 'f1',cv = 10)\n",
    "\n",
    "cv_score11 =(cv_score1+cv_score6)/2\n",
    "cv_score12 =(cv_score2+cv_score7)/2\n",
    "cv_score13 =(cv_score3+cv_score8)/2\n",
    "cv_score14 =(cv_score4+cv_score9)/2\n",
    "cv_score15 =(cv_score5+cv_score10)/2\n",
    "\n",
    "print('Test mix score based on 10-fold CV for Logistic Regression is      {}'.format(cv_score11.mean()))\n",
    "print('Test mix score based on 10-fold CV for Support Vector Classifier is {}'.format(cv_score12.mean()))\n",
    "print('Test mix score based on 10-fold CV for Random Forest Classifier is {}'.format(cv_score13.mean()))\n",
    "print('Test mix score based on 10-fold CV for kNearstNeibor is            {}'.format(cv_score14.mean()))\n",
    "print('Test mix score based on 10-fold CV for Gradient Boosted is         {}'.format(cv_score15.mean()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the 10-Fold accuracy and f1 score, the best model to determine whether or not the item is clothing is **Suport Vector Classifier**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Build Model for \"Fit\" category"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "relaxed               1794\n",
       "semifitted             925\n",
       "straightregular        690\n",
       "fittedtailored         568\n",
       "semi-fitted            280\n",
       "fitted / tailored      194\n",
       "oversized              181\n",
       "straight / regular      96\n",
       "Name: attribute_value, dtype: int64"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Inspect the tag name and distribution\n",
    "fit_tag['attribute_value'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# align the tag name\n",
    "fit_tag.loc[fit_tag['attribute_value']=='semifitted','attribute_value']='semi-fitted'\n",
    "fit_tag.loc[fit_tag['attribute_value']=='straightregular','attribute_value']='straight/regular'\n",
    "fit_tag.loc[fit_tag['attribute_value']=='straight / regular','attribute_value']='straight/regular'\n",
    "fit_tag.loc[fit_tag['attribute_value']=='fittedtailored','attribute_value']='fitted/tailored'\n",
    "fit_tag.loc[fit_tag['attribute_value']=='fitted / tailored','attribute_value']='fitted/tailored'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "relaxed             1794\n",
       "semi-fitted         1205\n",
       "straight/regular     786\n",
       "fitted/tailored      762\n",
       "oversized            181\n",
       "Name: attribute_value, dtype: int64"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Inspect the distribution of target\n",
    "fit_tag['attribute_value'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2949, 5)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fit_tag.drop_duplicates(subset='product_id').shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3040, 5)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fit_tag.drop_duplicates(subset=['product_id','attribute_value']).shape\n",
    "#some product have multiple tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>product_id</th>\n",
       "      <th>product_color_id</th>\n",
       "      <th>attribute_name</th>\n",
       "      <th>attribute_value</th>\n",
       "      <th>file</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>113116</th>\n",
       "      <td>01DPCHNQM0PA0SXZZZX85PF2ZJ</td>\n",
       "      <td>01DPCHNQM7FDGGJ2CKEKQWRKGS</td>\n",
       "      <td>fit</td>\n",
       "      <td>relaxed</td>\n",
       "      <td>additional</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82160</th>\n",
       "      <td>01DPCHNQM0PA0SXZZZX85PF2ZJ</td>\n",
       "      <td>01DPCHNQM7FDGGJ2CKEKQWRKGS</td>\n",
       "      <td>fit</td>\n",
       "      <td>straight/regular</td>\n",
       "      <td>additional</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>168</th>\n",
       "      <td>01DPGSTG4M1RXB26QMMN0MPPB8</td>\n",
       "      <td>01DPGSTMYC20XH5ZYK7S4XQEH5</td>\n",
       "      <td>fit</td>\n",
       "      <td>semi-fitted</td>\n",
       "      <td>initial_tags</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110482</th>\n",
       "      <td>01DPGSTG4M1RXB26QMMN0MPPB8</td>\n",
       "      <td>01DPGSV457VMYHWPYHW5V79JV9</td>\n",
       "      <td>fit</td>\n",
       "      <td>relaxed</td>\n",
       "      <td>additional</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104080</th>\n",
       "      <td>01DPGXGYVJ5DYV5G9CTAPGPFES</td>\n",
       "      <td>01DPGXHGB36NGP1E4PYFS7AYE2</td>\n",
       "      <td>fit</td>\n",
       "      <td>relaxed</td>\n",
       "      <td>additional</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7087</th>\n",
       "      <td>01DPGXGYVJ5DYV5G9CTAPGPFES</td>\n",
       "      <td>01DPGXHB9ZFSN94Z8PD5HKAW80</td>\n",
       "      <td>fit</td>\n",
       "      <td>semi-fitted</td>\n",
       "      <td>initial_tags</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77335</th>\n",
       "      <td>01DPKMXBJEMQWQWAZHGJK1RS8Z</td>\n",
       "      <td>01DPKMXBJJGRSZAPCX97AXGSN8</td>\n",
       "      <td>fit</td>\n",
       "      <td>fitted/tailored</td>\n",
       "      <td>additional</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113235</th>\n",
       "      <td>01DPKMXBJEMQWQWAZHGJK1RS8Z</td>\n",
       "      <td>01DPKMXBJJGRSZAPCX97AXGSN8</td>\n",
       "      <td>fit</td>\n",
       "      <td>semi-fitted</td>\n",
       "      <td>additional</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104022</th>\n",
       "      <td>01DS44MT6XDH3DDGJ80TQ6AJ2G</td>\n",
       "      <td>01DS44MT74BQ8X3QQ047Z8Q5HR</td>\n",
       "      <td>fit</td>\n",
       "      <td>oversized</td>\n",
       "      <td>additional</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3715</th>\n",
       "      <td>01DS44MT6XDH3DDGJ80TQ6AJ2G</td>\n",
       "      <td>01DS44MT74BQ8X3QQ047Z8Q5HR</td>\n",
       "      <td>fit</td>\n",
       "      <td>semi-fitted</td>\n",
       "      <td>initial_tags</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        product_id            product_color_id attribute_name  \\\n",
       "113116  01DPCHNQM0PA0SXZZZX85PF2ZJ  01DPCHNQM7FDGGJ2CKEKQWRKGS            fit   \n",
       "82160   01DPCHNQM0PA0SXZZZX85PF2ZJ  01DPCHNQM7FDGGJ2CKEKQWRKGS            fit   \n",
       "168     01DPGSTG4M1RXB26QMMN0MPPB8  01DPGSTMYC20XH5ZYK7S4XQEH5            fit   \n",
       "110482  01DPGSTG4M1RXB26QMMN0MPPB8  01DPGSV457VMYHWPYHW5V79JV9            fit   \n",
       "104080  01DPGXGYVJ5DYV5G9CTAPGPFES  01DPGXHGB36NGP1E4PYFS7AYE2            fit   \n",
       "7087    01DPGXGYVJ5DYV5G9CTAPGPFES  01DPGXHB9ZFSN94Z8PD5HKAW80            fit   \n",
       "77335   01DPKMXBJEMQWQWAZHGJK1RS8Z  01DPKMXBJJGRSZAPCX97AXGSN8            fit   \n",
       "113235  01DPKMXBJEMQWQWAZHGJK1RS8Z  01DPKMXBJJGRSZAPCX97AXGSN8            fit   \n",
       "104022  01DS44MT6XDH3DDGJ80TQ6AJ2G  01DS44MT74BQ8X3QQ047Z8Q5HR            fit   \n",
       "3715    01DS44MT6XDH3DDGJ80TQ6AJ2G  01DS44MT74BQ8X3QQ047Z8Q5HR            fit   \n",
       "\n",
       "         attribute_value          file  \n",
       "113116           relaxed    additional  \n",
       "82160   straight/regular    additional  \n",
       "168          semi-fitted  initial_tags  \n",
       "110482           relaxed    additional  \n",
       "104080           relaxed    additional  \n",
       "7087         semi-fitted  initial_tags  \n",
       "77335    fitted/tailored    additional  \n",
       "113235       semi-fitted    additional  \n",
       "104022         oversized    additional  \n",
       "3715         semi-fitted  initial_tags  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fit_nodup=fit_tag.drop_duplicates(subset=['product_id','attribute_value'])\n",
    "fit_nodup2=fit_tag.drop_duplicates(subset='product_id')\n",
    "\n",
    "#list of product that have multiple \"fit\" tags\n",
    "flist=fit_nodup[~(fit_nodup.index.isin(fit_nodup2.index))]['product_id'].values\n",
    "\n",
    "#Inspect the first 10 row (5 product) that have multi tags\n",
    "fit_nodup[fit_nodup['product_id'].isin(flist)].sort_values('product_id').head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop the product that have multi tag\n",
    "fit_tag2=fit_tag[~(fit_tag['product_id'].isin(flist))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "relaxed             1093\n",
       "semi-fitted          654\n",
       "straight/regular     553\n",
       "fitted/tailored      455\n",
       "oversized            105\n",
       "Name: attribute_value, dtype: int64"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Keep relevant column and remove duplicate record based on \"product_id\" and \"attribute_value\"\n",
    "fit_tagm=fit_tag2[['product_id','attribute_name','attribute_value']].\\\n",
    "drop_duplicates(subset=['product_id','attribute_value'])\n",
    "\n",
    "#remove duplicate record based on product id\n",
    "full_datam=full_data[['product_id','brand','brand_category','product_full_name','description','details']]\\\n",
    ".drop_duplicates(subset=['product_id'])\n",
    "fit_all=fit_tagm.merge(full_datam,how='left',on='product_id')\n",
    "#Fill Null value\n",
    "fit_all=fit_all.fillna('')\n",
    "\n",
    "#Concatnate string as input\n",
    "fit_all['input'] =fit_all[['brand','brand_category','product_full_name','description','details']]\\\n",
    ".agg(' '.join, axis=1).str.lower()\n",
    "\n",
    "fit_df=fit_all[['product_id','attribute_value','input']]\n",
    "\n",
    "#Inspect the distribution of target\n",
    "fit_df['attribute_value'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Text Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit_df['input']= fit_df['input'].apply(lem_sentences)\n",
    "fit_df['input'] = fit_df['input'].apply(cleanHtml)\n",
    "fit_df['input'] = fit_df['input'].apply(cleanPunc)\n",
    "fit_df['input'] = fit_df['input'].apply(keepAlpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save the dataframe for future usage\n",
    "fit_df.to_csv('fit_tags.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Model Building"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We used the OneVsRestClassifier() to build the multi-class classification model. The model would choose 1 of the 5 tags in \"Fit\" category as the predicted value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=fit_df['input']\n",
    "y=fit_df['attribute_value']\n",
    "X_train, X_test,y_train, y_test = train_test_split(X,y, random_state=42, test_size=0.30, shuffle=True,stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy for Logistic Regression is      0.6002331002331003\n",
      "Test accuracy for Support Vector Classifier is 0.6177156177156177\n",
      "Test accuracy for Random Forest Classifier is 0.6048951048951049\n",
      "Test accuracy for kNearstNeibor is            0.5512820512820513\n",
      "Test accuracy for Gradient Boosted is         0.6118881118881119\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Using pipeline for applying logistic regression, svc,randon forest classifier, kNN, and gradient boosted\n",
    "#Using both accuracy select model\n",
    "LogReg_pipeline = Pipeline([('tfidf', vectorizer),\n",
    "                ('clf', OneVsRestClassifier(LogisticRegression(solver='sag'), n_jobs=-1)),\n",
    "            ])\n",
    "\n",
    "SVC_pipeline = Pipeline([('tfidf', vectorizer),\n",
    "                ('clf', OneVsRestClassifier(LinearSVC())),\n",
    "            ])\n",
    "\n",
    "RandomFC_pipeline = Pipeline([('tfidf', vectorizer),\n",
    "                ('clf', OneVsRestClassifier(RandomForestClassifier())),\n",
    "            ])\n",
    "\n",
    "kNN_pipeline = Pipeline([('tfidf', vectorizer),\n",
    "                ('clf', OneVsRestClassifier(KNeighborsClassifier())),\n",
    "            ])\n",
    "\n",
    "GradientFC_pipeline = Pipeline([('tfidf', vectorizer),\n",
    "                ('clf', OneVsRestClassifier(GradientBoostingClassifier())),\n",
    "            ])\n",
    "\n",
    "\n",
    "LogReg_pipeline.fit(X_train, y_train)\n",
    "SVC_pipeline.fit(X_train, y_train)\n",
    "RandomFC_pipeline.fit(X_train, y_train)\n",
    "kNN_pipeline.fit(X_train, y_train)\n",
    "GradientFC_pipeline.fit(X_train, y_train)\n",
    "    \n",
    "    # calculating test accuracy\n",
    "prediction1 = LogReg_pipeline.predict(X_test)\n",
    "prediction2 = SVC_pipeline.predict(X_test)\n",
    "prediction3 = RandomFC_pipeline.predict(X_test)\n",
    "prediction4 = kNN_pipeline.predict(X_test)\n",
    "prediction5 = GradientFC_pipeline.predict(X_test)\n",
    "    \n",
    "accuracy1=accuracy_score(y_test, prediction1)\n",
    "accuracy2=accuracy_score(y_test, prediction2)\n",
    "accuracy3=accuracy_score(y_test, prediction3)\n",
    "accuracy4=accuracy_score(y_test, prediction4)\n",
    "accuracy5=accuracy_score(y_test, prediction5)\n",
    "    \n",
    "print('Test accuracy for Logistic Regression is      {}'.format(accuracy1))\n",
    "print('Test accuracy for Support Vector Classifier is {}'.format(accuracy2))\n",
    "print('Test accuracy for Random Forest Classifier is {}'.format(accuracy3))\n",
    "print('Test accuracy for kNearstNeibor is            {}'.format(accuracy4))\n",
    "print('Test accuracy for Gradient Boosted is         {}'.format(accuracy5))\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **Support Vector Classifer** model seems to perform the best, but we should cross validate to see if the model performance is consistent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 10-Fold Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy based on 10-fold CV for Logistic Regression is      0.5674825174825175\n",
      "Test accuracy based on 10-fold CV for Suport Vector Classifier is 0.5811188811188812\n",
      "Test accuracy based on 10-fold CV for Random Forest Classifier is 0.565034965034965\n",
      "Test accuracy based on 10-fold CV for kNearstNeibor is            0.5083916083916085\n",
      "Test accuracy based on 10-fold CV for Gradient Boosted is         0.5818181818181818\n"
     ]
    }
   ],
   "source": [
    "X=fit_df['input']\n",
    "y=fit_df['attribute_value']\n",
    "\n",
    "LogReg_pipeline = Pipeline([('tfidf', vectorizer),\n",
    "                ('clf', OneVsRestClassifier(LogisticRegression(solver='sag'), n_jobs=-1)),\n",
    "            ])\n",
    "\n",
    "SVC_pipeline = Pipeline([('tfidf', vectorizer),\n",
    "                ('clf', OneVsRestClassifier(LinearSVC())),\n",
    "            ])\n",
    "\n",
    "RandomFC_pipeline = Pipeline([('tfidf', vectorizer),\n",
    "                ('clf', OneVsRestClassifier(RandomForestClassifier())),\n",
    "            ])\n",
    "\n",
    "kNN_pipeline = Pipeline([('tfidf', vectorizer),\n",
    "                ('clf', OneVsRestClassifier(KNeighborsClassifier())),\n",
    "            ])\n",
    "\n",
    "GradientFC_pipeline = Pipeline([('tfidf', vectorizer),\n",
    "                ('clf', OneVsRestClassifier(GradientBoostingClassifier())),\n",
    "            ])\n",
    "\n",
    "cv_score1 =cross_val_score(LogReg_pipeline, X, y, scoring = 'accuracy',cv = 10)\n",
    "cv_score2 =cross_val_score(SVC_pipeline, X, y, scoring = 'accuracy',cv = 10)\n",
    "cv_score3 =cross_val_score(RandomFC_pipeline, X, y, scoring = 'accuracy',cv = 10)\n",
    "cv_score4 =cross_val_score(kNN_pipeline, X, y, scoring = 'accuracy',cv = 10)\n",
    "cv_score5 =cross_val_score(GradientFC_pipeline, X, y, scoring = 'accuracy',cv = 10)\n",
    "\n",
    "print('Test accuracy based on 10-fold CV for Logistic Regression is      {}'.format(cv_score1.mean()))\n",
    "print('Test accuracy based on 10-fold CV for Support Vector Classifier is {}'.format(cv_score2.mean()))\n",
    "print('Test accuracy based on 10-fold CV for Random Forest Classifier is {}'.format(cv_score3.mean()))\n",
    "print('Test accuracy based on 10-fold CV for kNearstNeibor is            {}'.format(cv_score4.mean()))\n",
    "print('Test accuracy based on 10-fold CV for Gradient Boosted is         {}'.format(cv_score5.mean()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the 10-Fold accuracy, the best modedl to determine which \"Fit\" tag an clothing item should have is **Gradient Boosted Classifier**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Build Model for \"Dry Clean Only\" tag"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "no     2182\n",
       "yes    2158\n",
       "Name: attribute_value, dtype: int64"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Inpect the tag name and ditribution\n",
    "dryclean_tag['attribute_value'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2776, 5)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dryclean_tag.drop_duplicates(subset='product_id').shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2779, 5)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dryclean_tag.drop_duplicates(subset=['product_id','attribute_value']).shape\n",
    "#there are product with multitags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>product_id</th>\n",
       "      <th>product_color_id</th>\n",
       "      <th>attribute_name</th>\n",
       "      <th>attribute_value</th>\n",
       "      <th>file</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>16870</th>\n",
       "      <td>01DT50VPJ23TNSDY43EQ54CGSK</td>\n",
       "      <td>01DT50VPJ8TJNDBGQPGDRZZJN5</td>\n",
       "      <td>dry_clean_only</td>\n",
       "      <td>yes</td>\n",
       "      <td>initial_tags</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89393</th>\n",
       "      <td>01DT50VPJ23TNSDY43EQ54CGSK</td>\n",
       "      <td>01DT50VPJ8TJNDBGQPGDRZZJN5</td>\n",
       "      <td>dry_clean_only</td>\n",
       "      <td>no</td>\n",
       "      <td>additional</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111900</th>\n",
       "      <td>01E1JKV4WQYPMJYVXYNN4NVYK8</td>\n",
       "      <td>01E1JKV4X22KTME0JSZMC7G30G</td>\n",
       "      <td>dry_clean_only</td>\n",
       "      <td>yes</td>\n",
       "      <td>additional</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112230</th>\n",
       "      <td>01E1JKV4WQYPMJYVXYNN4NVYK8</td>\n",
       "      <td>01E1JKV4X22KTME0JSZMC7G30G</td>\n",
       "      <td>dry_clean_only</td>\n",
       "      <td>no</td>\n",
       "      <td>additional</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55344</th>\n",
       "      <td>01E4EAG0KNT88YFSPB3FGSXAH8</td>\n",
       "      <td>01E4EAG0M0PKPE4TDCAPFK4C2R</td>\n",
       "      <td>dry_clean_only</td>\n",
       "      <td>no</td>\n",
       "      <td>additional</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55706</th>\n",
       "      <td>01E4EAG0KNT88YFSPB3FGSXAH8</td>\n",
       "      <td>01E4EAG0M0PKPE4TDCAPFK4C2R</td>\n",
       "      <td>dry_clean_only</td>\n",
       "      <td>yes</td>\n",
       "      <td>additional</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        product_id            product_color_id  \\\n",
       "16870   01DT50VPJ23TNSDY43EQ54CGSK  01DT50VPJ8TJNDBGQPGDRZZJN5   \n",
       "89393   01DT50VPJ23TNSDY43EQ54CGSK  01DT50VPJ8TJNDBGQPGDRZZJN5   \n",
       "111900  01E1JKV4WQYPMJYVXYNN4NVYK8  01E1JKV4X22KTME0JSZMC7G30G   \n",
       "112230  01E1JKV4WQYPMJYVXYNN4NVYK8  01E1JKV4X22KTME0JSZMC7G30G   \n",
       "55344   01E4EAG0KNT88YFSPB3FGSXAH8  01E4EAG0M0PKPE4TDCAPFK4C2R   \n",
       "55706   01E4EAG0KNT88YFSPB3FGSXAH8  01E4EAG0M0PKPE4TDCAPFK4C2R   \n",
       "\n",
       "        attribute_name attribute_value          file  \n",
       "16870   dry_clean_only             yes  initial_tags  \n",
       "89393   dry_clean_only              no    additional  \n",
       "111900  dry_clean_only             yes    additional  \n",
       "112230  dry_clean_only              no    additional  \n",
       "55344   dry_clean_only              no    additional  \n",
       "55706   dry_clean_only             yes    additional  "
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dryclean_nodup=dryclean_tag.drop_duplicates(subset=['product_id','attribute_value'])\n",
    "dryclean_nodup2=dryclean_tag.drop_duplicates(subset='product_id')\n",
    "\n",
    "#list of product that have multi tag\n",
    "dclist=dryclean_nodup[~(dryclean_nodup.index.isin(dryclean_nodup2.index))]['product_id'].values\n",
    "\n",
    "dryclean_nodup[dryclean_nodup['product_id'].isin(dclist)].sort_values('product_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove the record that have contradicting target value\n",
    "dryclean_tag2=dryclean_tag[~(dryclean_tag['product_id'].isin(dclist))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "yes    1421\n",
       "no     1352\n",
       "Name: attribute_value, dtype: int64"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Keep relevant column and remove duplicate record based on \"product_id\" and \"attribute_value\"\n",
    "dryclean_tagm=dryclean_tag2[['product_id','attribute_name','attribute_value']].\\\n",
    "drop_duplicates(subset=['product_id','attribute_value'])\n",
    "\n",
    "#remove duplicate record based on product id\n",
    "full_datam=full_data[['product_id','brand','brand_category','product_full_name','description','details']]\\\n",
    ".drop_duplicates(subset=['product_id'])\n",
    "dryclean_all=dryclean_tagm.merge(full_datam,how='left',on='product_id')\n",
    "#Fill Null value\n",
    "dryclean_all=dryclean_all.fillna('')\n",
    "\n",
    "#Concatnate string as input\n",
    "dryclean_all['input'] =dryclean_all[['brand','brand_category','product_full_name','description','details']]\\\n",
    ".agg(' '.join, axis=1).str.lower()\n",
    "\n",
    "dryclean_df=dryclean_all[['product_id','attribute_value','input']]\n",
    "dryclean_df['attribute_value'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert the target value to 1 and 0\n",
    "# this step help us compute f1 score latter\n",
    "dryclean_df['dry_clean']=np.where(dryclean_df['attribute_value']=='yes',1,0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Text Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "dryclean_df['input']= dryclean_df['input'].apply(lem_sentences)\n",
    "dryclean_df['input'] = dryclean_df['input'].apply(cleanHtml)\n",
    "dryclean_df['input'] = dryclean_df['input'].apply(cleanPunc)\n",
    "dryclean_df['input'] = dryclean_df['input'].apply(keepAlpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save the dataframe for future use\n",
    "dryclean_df.to_csv('dryclean_tags.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Model Building"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is also a binary classification model where 1 represents the item is dry-clean-only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=dryclean_df['input']\n",
    "y=dryclean_df['dry_clean']\n",
    "X_train, X_test,y_train, y_test = train_test_split(X,y, random_state=42, test_size=0.30, shuffle=True,stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy for Logistic Regression is      0.8966346153846154\n",
      "Test accuracy for Support Vector Classifier is 0.8786057692307693\n",
      "Test accuracy for Random Forest Classifier is 0.8978365384615384\n",
      "Test accuracy for kNearstNeibor is            0.859375\n",
      "Test accuracy for Gradient Boosted is         0.8966346153846154\n",
      "\n",
      "\n",
      "Test f1_score for Logistic Regression is      0.9024943310657597\n",
      "Test f1_score for Support Vector Classifier is 0.8845714285714286\n",
      "Test f1_score for Random Forest Classifier is 0.9017341040462429\n",
      "Test f1_score for kNearstNeibor is            0.8674971687429219\n",
      "Test f1_score for Gradient Boosted is         0.8990610328638498\n",
      "\n",
      "\n",
      "Test mix score for Logistic Regression is      0.8995644732251875\n",
      "Test mix score for Support Vector Classifier is 0.881588598901099\n",
      "Test mix score for Random Forest Classifier is 0.8997853212538907\n",
      "Test mix score for kNearstNeibor is            0.863436084371461\n",
      "Test mix score for Gradient Boosted is         0.8978478241242326\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Using pipeline for applying logistic regression, svc,randon forest classifier, kNN, and gradient boosted\n",
    "#Using both accuracy and f1 score to select model\n",
    "LogReg_pipeline = Pipeline([('tfidf', vectorizer),\n",
    "                ('clf', LogisticRegression(solver='sag')),\n",
    "            ])\n",
    "\n",
    "SVC_pipeline = Pipeline([('tfidf', vectorizer),\n",
    "                ('clf', LinearSVC()),\n",
    "            ])\n",
    "\n",
    "RandomFC_pipeline = Pipeline([('tfidf', vectorizer),\n",
    "                ('clf', RandomForestClassifier()),\n",
    "            ])\n",
    "\n",
    "kNN_pipeline = Pipeline([('tfidf', vectorizer),\n",
    "                ('clf', KNeighborsClassifier()),\n",
    "            ])\n",
    "\n",
    "GradientFC_pipeline = Pipeline([('tfidf', vectorizer),\n",
    "                ('clf', GradientBoostingClassifier()),\n",
    "            ])\n",
    "\n",
    "\n",
    "LogReg_pipeline.fit(X_train, y_train)\n",
    "SVC_pipeline.fit(X_train, y_train)\n",
    "RandomFC_pipeline.fit(X_train, y_train)\n",
    "kNN_pipeline.fit(X_train, y_train)\n",
    "GradientFC_pipeline.fit(X_train, y_train)\n",
    "    \n",
    "    # calculating test accuracy\n",
    "prediction1 = LogReg_pipeline.predict(X_test)\n",
    "prediction2 = SVC_pipeline.predict(X_test)\n",
    "prediction3 = RandomFC_pipeline.predict(X_test)\n",
    "prediction4 = kNN_pipeline.predict(X_test)\n",
    "prediction5 = GradientFC_pipeline.predict(X_test)\n",
    "    \n",
    "accuracy1=accuracy_score(y_test, prediction1)\n",
    "accuracy2=accuracy_score(y_test, prediction2)\n",
    "accuracy3=accuracy_score(y_test, prediction3)\n",
    "accuracy4=accuracy_score(y_test, prediction4)\n",
    "accuracy5=accuracy_score(y_test, prediction5)\n",
    "    \n",
    "f1_score1=f1_score(y_test, prediction1)\n",
    "f1_score2=f1_score(y_test, prediction2)\n",
    "f1_score3=f1_score(y_test, prediction3)\n",
    "f1_score4=f1_score(y_test, prediction4)\n",
    "f1_score5=f1_score(y_test, prediction5)\n",
    "\n",
    "print('Test accuracy for Logistic Regression is      {}'.format(accuracy1))\n",
    "print('Test accuracy for Support Vector Classifier is {}'.format(accuracy2))\n",
    "print('Test accuracy for Random Forest Classifier is {}'.format(accuracy3))\n",
    "print('Test accuracy for kNearstNeibor is            {}'.format(accuracy4))\n",
    "print('Test accuracy for Gradient Boosted is         {}'.format(accuracy5))\n",
    "print(\"\\n\")\n",
    "print('Test f1_score for Logistic Regression is      {}'.format(f1_score1))\n",
    "print('Test f1_score for Support Vector Classifier is {}'.format(f1_score2))\n",
    "print('Test f1_score for Random Forest Classifier is {}'.format(f1_score3))\n",
    "print('Test f1_score for kNearstNeibor is            {}'.format(f1_score4))\n",
    "print('Test f1_score for Gradient Boosted is         {}'.format(f1_score5))\n",
    "print(\"\\n\")\n",
    "print('Test mix score for Logistic Regression is      {}'.format((accuracy1+f1_score1)/2))\n",
    "print('Test mix score for Support Vector Classifier is {}'.format((accuracy2+f1_score2)/2))\n",
    "print('Test mix score for Random Forest Classifier is {}'.format((accuracy3+f1_score3)/2))\n",
    "print('Test mix score for kNearstNeibor is            {}'.format((accuracy4+f1_score4)/2))\n",
    "print('Test mix score for Gradient Boosted is         {}'.format((accuracy5+f1_score5)/2))\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **Random Forest Classifier** model seems to perform the best, but we should cross validate to see if the model performance is consistent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 10-Fold Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test mix score based on 10-fold CV for Logistic Regression is      0.8662928038760057\n",
      "Test mix score based on 10-fold CV for Suport Vector Classifier is 0.8576460160455277\n",
      "Test mix score based on 10-fold CV for Random Forest Classifier is 0.8557361085966676\n",
      "Test mix score based on 10-fold CV for kNearstNeibor is            0.809174943570073\n",
      "Test mix score based on 10-fold CV for Gradient Boosted is         0.8674092764188458\n"
     ]
    }
   ],
   "source": [
    "LogReg_pipeline = Pipeline([('tfidf', vectorizer),\n",
    "                ('clf', LogisticRegression(solver='sag')),])\n",
    "\n",
    "SVC_pipeline = Pipeline([('tfidf', vectorizer),\n",
    "                ('clf', LinearSVC()),\n",
    "            ])\n",
    "\n",
    "RandomFC_pipeline = Pipeline([('tfidf', vectorizer),\n",
    "                ('clf', RandomForestClassifier()),\n",
    "            ])\n",
    "\n",
    "kNN_pipeline = Pipeline([('tfidf', vectorizer),\n",
    "                ('clf', KNeighborsClassifier()),\n",
    "            ])\n",
    "\n",
    "GradientFC_pipeline = Pipeline([('tfidf', vectorizer),\n",
    "                ('clf', GradientBoostingClassifier()),\n",
    "            ])\n",
    "\n",
    "X=dryclean_df['input']\n",
    "y=dryclean_df['dry_clean']\n",
    "\n",
    "cv_score1 =cross_val_score(LogReg_pipeline, X, y, scoring = 'accuracy',cv = 10)\n",
    "cv_score2 =cross_val_score(SVC_pipeline, X, y, scoring = 'accuracy',cv = 10)\n",
    "cv_score3 =cross_val_score(RandomFC_pipeline, X, y, scoring = 'accuracy',cv = 10)\n",
    "cv_score4 =cross_val_score(kNN_pipeline, X, y, scoring = 'accuracy',cv = 10)\n",
    "cv_score5 =cross_val_score(GradientFC_pipeline, X, y, scoring = 'accuracy',cv = 10)\n",
    "\n",
    "cv_score6 =cross_val_score(LogReg_pipeline, X, y, scoring = 'f1',cv = 10)\n",
    "cv_score7 =cross_val_score(SVC_pipeline, X, y, scoring = 'f1',cv = 10)\n",
    "cv_score8 =cross_val_score(RandomFC_pipeline, X, y, scoring = 'f1',cv = 10)\n",
    "cv_score9 =cross_val_score(kNN_pipeline, X, y, scoring = 'f1',cv = 10)\n",
    "cv_score10 =cross_val_score(GradientFC_pipeline, X, y, scoring = 'f1',cv = 10)\n",
    "\n",
    "cv_score11 =(cv_score1+cv_score6)/2\n",
    "cv_score12 =(cv_score2+cv_score7)/2\n",
    "cv_score13 =(cv_score3+cv_score8)/2\n",
    "cv_score14 =(cv_score4+cv_score9)/2\n",
    "cv_score15 =(cv_score5+cv_score10)/2\n",
    "\n",
    "print('Test mix score based on 10-fold CV for Logistic Regression is      {}'.format(cv_score11.mean()))\n",
    "print('Test mix score based on 10-fold CV for Support Vector Classifier is {}'.format(cv_score12.mean()))\n",
    "print('Test mix score based on 10-fold CV for Random Forest Classifier is {}'.format(cv_score13.mean()))\n",
    "print('Test mix score based on 10-fold CV for kNearstNeibor is            {}'.format(cv_score14.mean()))\n",
    "print('Test mix score based on 10-fold CV for Gradient Boosted is         {}'.format(cv_score15.mean()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the 10-Fold accuracy and f1 score, the best model to determine whether or not an clothing item is dry-clean only is **Gradient Boosted Classifer**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Build Model for \"Category\" tag"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "top                      1524\n",
       "shoe                     1460\n",
       "bottom                   1365\n",
       "accessory                 640\n",
       "one piece                 605\n",
       "sweater                   545\n",
       "blazers_coats_jackets     433\n",
       "sweatshirt_hoodie         208\n",
       "Name: attribute_value, dtype: int64"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "category_tag['attribute_value'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "#group top clothing into the same tag\n",
    "top_list=['top','sweater','blazers_coats_jackets','sweatshirt_hoodie']\n",
    "category_tag['attribute_category']=category_tag['attribute_value'].apply(is_top)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "top          2710\n",
       "shoe         1460\n",
       "bottom       1365\n",
       "accessory     640\n",
       "one piece     605\n",
       "Name: attribute_category, dtype: int64"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "category_tag['attribute_category'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>product_id</th>\n",
       "      <th>product_color_id</th>\n",
       "      <th>attribute_name</th>\n",
       "      <th>attribute_value</th>\n",
       "      <th>file</th>\n",
       "      <th>clothing</th>\n",
       "      <th>attribute_category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>77339</th>\n",
       "      <td>01DPKMXBJEMQWQWAZHGJK1RS8Z</td>\n",
       "      <td>01DPKMXBJJGRSZAPCX97AXGSN8</td>\n",
       "      <td>category</td>\n",
       "      <td>top</td>\n",
       "      <td>additional</td>\n",
       "      <td>1</td>\n",
       "      <td>top</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113233</th>\n",
       "      <td>01DPKMXBJEMQWQWAZHGJK1RS8Z</td>\n",
       "      <td>01DPKMXBJJGRSZAPCX97AXGSN8</td>\n",
       "      <td>category</td>\n",
       "      <td>bottom</td>\n",
       "      <td>additional</td>\n",
       "      <td>1</td>\n",
       "      <td>bottom</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3873</th>\n",
       "      <td>01DSP2PPDS647HVVN1GAA1F68Q</td>\n",
       "      <td>01DSP2PPE1RBXCBN4D0HFJCH49</td>\n",
       "      <td>category</td>\n",
       "      <td>top</td>\n",
       "      <td>initial_tags</td>\n",
       "      <td>1</td>\n",
       "      <td>top</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82153</th>\n",
       "      <td>01DSP2PPDS647HVVN1GAA1F68Q</td>\n",
       "      <td>01DSP2PPE1RBXCBN4D0HFJCH49</td>\n",
       "      <td>category</td>\n",
       "      <td>accessory</td>\n",
       "      <td>additional</td>\n",
       "      <td>0</td>\n",
       "      <td>accessory</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8323</th>\n",
       "      <td>01DT515KEMTBG2ZTNRHYQ29PCT</td>\n",
       "      <td>01DT515KET8PG0ATT955Q3N0VX</td>\n",
       "      <td>category</td>\n",
       "      <td>accessory</td>\n",
       "      <td>initial_tags</td>\n",
       "      <td>0</td>\n",
       "      <td>accessory</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75400</th>\n",
       "      <td>01DT515KEMTBG2ZTNRHYQ29PCT</td>\n",
       "      <td>01DT515KET8PG0ATT955Q3N0VX</td>\n",
       "      <td>category</td>\n",
       "      <td>top</td>\n",
       "      <td>additional</td>\n",
       "      <td>1</td>\n",
       "      <td>top</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1683</th>\n",
       "      <td>01DTJAZ7F9DRFPNF8W1H7SFT20</td>\n",
       "      <td>01DTJAZ7FJVFXV2QWZVSZ4RNMY</td>\n",
       "      <td>category</td>\n",
       "      <td>top</td>\n",
       "      <td>initial_tags</td>\n",
       "      <td>1</td>\n",
       "      <td>top</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98437</th>\n",
       "      <td>01DTJAZ7F9DRFPNF8W1H7SFT20</td>\n",
       "      <td>01DTJAZ7FJVFXV2QWZVSZ4RNMY</td>\n",
       "      <td>category</td>\n",
       "      <td>accessory</td>\n",
       "      <td>additional</td>\n",
       "      <td>0</td>\n",
       "      <td>accessory</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        product_id            product_color_id attribute_name  \\\n",
       "77339   01DPKMXBJEMQWQWAZHGJK1RS8Z  01DPKMXBJJGRSZAPCX97AXGSN8       category   \n",
       "113233  01DPKMXBJEMQWQWAZHGJK1RS8Z  01DPKMXBJJGRSZAPCX97AXGSN8       category   \n",
       "3873    01DSP2PPDS647HVVN1GAA1F68Q  01DSP2PPE1RBXCBN4D0HFJCH49       category   \n",
       "82153   01DSP2PPDS647HVVN1GAA1F68Q  01DSP2PPE1RBXCBN4D0HFJCH49       category   \n",
       "8323    01DT515KEMTBG2ZTNRHYQ29PCT  01DT515KET8PG0ATT955Q3N0VX       category   \n",
       "75400   01DT515KEMTBG2ZTNRHYQ29PCT  01DT515KET8PG0ATT955Q3N0VX       category   \n",
       "1683    01DTJAZ7F9DRFPNF8W1H7SFT20  01DTJAZ7FJVFXV2QWZVSZ4RNMY       category   \n",
       "98437   01DTJAZ7F9DRFPNF8W1H7SFT20  01DTJAZ7FJVFXV2QWZVSZ4RNMY       category   \n",
       "\n",
       "       attribute_value          file  clothing attribute_category  \n",
       "77339              top    additional         1                top  \n",
       "113233          bottom    additional         1             bottom  \n",
       "3873               top  initial_tags         1                top  \n",
       "82153        accessory    additional         0          accessory  \n",
       "8323         accessory  initial_tags         0          accessory  \n",
       "75400              top    additional         1                top  \n",
       "1683               top  initial_tags         1                top  \n",
       "98437        accessory    additional         0          accessory  "
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Inspect data to find item with mupltiple tags\n",
    "tdf3=category_tag.drop_duplicates(subset=['product_id','attribute_category'])\n",
    "tdf4=category_tag.drop_duplicates(subset='product_id')\n",
    "\n",
    "tlist2=tdf3[~(tdf3.index.isin(tdf4.index))]['product_id'].values\n",
    "tdf3[tdf3['product_id'].isin(tlist2)].sort_values('product_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Exclude these record\n",
    "category_tag3=category_tag[~(category_tag['product_id'].isin(tlist2))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Keep relevant column and remove duplicate record based on \"product_id\" and \"attribute_value\"\n",
    "category_tagm2=category_tag3[['product_id','attribute_name','attribute_category']].\\\n",
    "drop_duplicates(subset=['product_id','attribute_category'])\n",
    "\n",
    "#remove duplicate record based on product id\n",
    "full_datam=full_data[['product_id','brand','brand_category','product_full_name','description','details']]\\\n",
    ".drop_duplicates(subset=['product_id'])\n",
    "category_all2=category_tagm2.merge(full_datam,how='left',on='product_id')\n",
    "#Fill Null value\n",
    "category_all2=category_all2.fillna('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "top          1657\n",
       "bottom        899\n",
       "shoe          672\n",
       "one piece     427\n",
       "accessory     310\n",
       "Name: attribute_category, dtype: int64"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Inpect distribution\n",
    "category_all2['attribute_category'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Exclude the \"accessory\" type since we would grab some data from the full_data\n",
    "#to generate record for \"handbag\", \"scarf\", and \"other(including sunglasses, belt, case etc)\"\n",
    "category_part=category_all2[category_all2['attribute_category']!='accessory']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "#generate data for the target value \"handbag\" from full_data\n",
    "#using rule-based method on \"product_full_name\" to find target\n",
    "handbag_tag=full_data[full_data['product_full_name'].fillna('').str.lower().str.contains(r'bag|clutch|purse')]\\\n",
    ".drop_duplicates(subset='product_id')\n",
    "\n",
    "handbag_tag['attribute_name']='category'\n",
    "handbag_tag['attribute_category']='handbag'\n",
    "handbag_tag=handbag_tag[['product_id','attribute_name','attribute_category',\n",
    "                         'brand','brand_category','product_full_name','description','details']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "#generate data for the target value \"scarf\" from full_data\n",
    "scarf_tag=full_data[full_data['product_full_name'].fillna('').str.lower().str.contains(r'scarf|scarves')]\\\n",
    ".drop_duplicates(subset='product_id')\n",
    "\n",
    "scarf_tag['attribute_name']='category'\n",
    "scarf_tag['attribute_category']='scarf'\n",
    "scarf_tag=scarf_tag[['product_id','attribute_name','attribute_category',\n",
    "                         'brand','brand_category','product_full_name','description','details']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "#generate data for the target value \"other\" from full_data\n",
    "#sampling 1000 out of over 3000 record to make the distribution of target value more balance\n",
    "other_tag=full_data[full_data['product_full_name'].fillna('').str.lower().str.contains(r'case|accessory|belt|sunglasses')]\\\n",
    ".drop_duplicates(subset='product_id').sample(1000,random_state=42)\n",
    "other_tag['attribute_name']='category'\n",
    "other_tag['attribute_category']='other'\n",
    "other_tag=other_tag[['product_id','attribute_name','attribute_category',\n",
    "                         'brand','brand_category','product_full_name','description','details']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "#concat the above 4 dataframe\n",
    "category_all2=pd.concat([category_part,handbag_tag,scarf_tag,other_tag])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Investigate if there are multiple tag under the same product\n",
    "tlist3=category_all2[category_all2.duplicated(subset='product_id')]['product_id'].values\n",
    "\n",
    "#the rule_based method may not be very solid\n",
    "#if an item have multiple tag because of the data extracted from full_data\n",
    "#consider the data in tagged_data the true value\n",
    "add_list=['other','scarf','handbag']\n",
    "true_category=category_all2[(~(category_all2['attribute_category'].isin(add_list)))&(category_all2['product_id'].isin(tlist3))]\n",
    "\n",
    "#exclude the product with multiple tag\n",
    "category_all2=category_all2[~(category_all2['product_id'].isin(tlist3))]\n",
    "#add back the product with true value\n",
    "category_all2=pd.concat([category_all2,true_category])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "top          1657\n",
       "other         960\n",
       "bottom        899\n",
       "scarf         738\n",
       "handbag       710\n",
       "shoe          672\n",
       "one piece     427\n",
       "Name: attribute_category, dtype: int64"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Inspect the dataframe\n",
    "category_all2['attribute_category'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Concatnate string as input\n",
    "category_all2=category_all2.fillna('')\n",
    "category_all2['input'] =category_all2[['brand','brand_category','product_full_name','description','details']].astype('str')\\\n",
    ".agg(' '.join, axis=1).str.lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Text Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "category_all2['input']= category_all2['input'].apply(lem_sentences)\n",
    "category_all2['input'] = category_all2['input'].apply(cleanHtml)\n",
    "category_all2['input'] = category_all2['input'].apply(cleanPunc)\n",
    "category_all2['input'] = category_all2['input'].apply(keepAlpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6063, 9)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "category_all2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save the dataframe for future usage\n",
    "category_all2.to_csv('category_tags.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Model Building"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Same as in \"fit\" tag, we used the OneVsRestClassifier() to build the multi-class classification model to choose 1 tag out 7 as the predicted value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=category_all2['input']\n",
    "y=category_all2['attribute_category']\n",
    "X_train, X_test,y_train, y_test = train_test_split(X,y, random_state=42, test_size=0.30, shuffle=True,stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy for Logistic Regression is      0.9681143485431556\n",
      "Test accuracy for Support Vector Classifier is 0.9879054425508521\n",
      "Test accuracy for Random Forest Classifier is 0.960417811984607\n",
      "Test accuracy for kNearstNeibor is            0.9241341396371633\n",
      "Test accuracy for Gradient Boosted is         0.9758108851017042\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Using pipeline for applying logistic regression, svc,randon forest classifier, kNN, and gradient boosted\n",
    "#Using accuracy to select model\n",
    "LogReg_pipeline = Pipeline([('tfidf', vectorizer),\n",
    "                ('clf', OneVsRestClassifier(LogisticRegression(solver='sag'), n_jobs=-1)),\n",
    "            ])\n",
    "\n",
    "SVC_pipeline = Pipeline([('tfidf', vectorizer),\n",
    "                ('clf', OneVsRestClassifier(LinearSVC())),\n",
    "            ])\n",
    "\n",
    "RandomFC_pipeline = Pipeline([('tfidf', vectorizer),\n",
    "                ('clf', OneVsRestClassifier(RandomForestClassifier())),\n",
    "            ])\n",
    "\n",
    "kNN_pipeline = Pipeline([('tfidf', vectorizer),\n",
    "                ('clf', OneVsRestClassifier(KNeighborsClassifier())),\n",
    "            ])\n",
    "\n",
    "GradientFC_pipeline = Pipeline([('tfidf', vectorizer),\n",
    "                ('clf', OneVsRestClassifier(GradientBoostingClassifier())),\n",
    "            ])\n",
    "\n",
    "\n",
    "LogReg_pipeline.fit(X_train, y_train)\n",
    "SVC_pipeline.fit(X_train, y_train)\n",
    "RandomFC_pipeline.fit(X_train, y_train)\n",
    "kNN_pipeline.fit(X_train, y_train)\n",
    "GradientFC_pipeline.fit(X_train, y_train)\n",
    "    \n",
    "    # calculating test accuracy\n",
    "prediction1 = LogReg_pipeline.predict(X_test)\n",
    "prediction2 = SVC_pipeline.predict(X_test)\n",
    "prediction3 = RandomFC_pipeline.predict(X_test)\n",
    "prediction4 = kNN_pipeline.predict(X_test)\n",
    "prediction5 = GradientFC_pipeline.predict(X_test)\n",
    "    \n",
    "accuracy1=accuracy_score(y_test, prediction1)\n",
    "accuracy2=accuracy_score(y_test, prediction2)\n",
    "accuracy3=accuracy_score(y_test, prediction3)\n",
    "accuracy4=accuracy_score(y_test, prediction4)\n",
    "accuracy5=accuracy_score(y_test, prediction5)\n",
    "    \n",
    "print('Test accuracy for Logistic Regression is      {}'.format(accuracy1))\n",
    "print('Test accuracy for Support Vector Classifier is {}'.format(accuracy2))\n",
    "print('Test accuracy for Random Forest Classifier is {}'.format(accuracy3))\n",
    "print('Test accuracy for kNearstNeibor is            {}'.format(accuracy4))\n",
    "print('Test accuracy for Gradient Boosted is         {}'.format(accuracy5))\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **Support Vector Classifier** model seems to perform the best, but we should cross validate to see if the model performance is consistent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 10-Fold Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy based on 10-fold CV for Logistic Regression is      0.9661873848010831\n",
      "Test accuracy based on 10-fold CV for Suport Vector Classifier is 0.981363193980024\n",
      "Test accuracy based on 10-fold CV for Random Forest Classifier is 0.9572794841263368\n",
      "Test accuracy based on 10-fold CV for kNearstNeibor is            0.9017001864931139\n",
      "Test accuracy based on 10-fold CV for Gradient Boosted is         0.9731169904469853\n"
     ]
    }
   ],
   "source": [
    "X=category_all2['input']\n",
    "y=category_all2['attribute_category']\n",
    "\n",
    "LogReg_pipeline = Pipeline([('tfidf', vectorizer),\n",
    "                ('clf', OneVsRestClassifier(LogisticRegression(solver='sag'), n_jobs=-1)),\n",
    "            ])\n",
    "\n",
    "SVC_pipeline = Pipeline([('tfidf', vectorizer),\n",
    "                ('clf', OneVsRestClassifier(LinearSVC())),\n",
    "            ])\n",
    "\n",
    "RandomFC_pipeline = Pipeline([('tfidf', vectorizer),\n",
    "                ('clf', OneVsRestClassifier(RandomForestClassifier())),\n",
    "            ])\n",
    "\n",
    "kNN_pipeline = Pipeline([('tfidf', vectorizer),\n",
    "                ('clf', OneVsRestClassifier(KNeighborsClassifier())),\n",
    "            ])\n",
    "\n",
    "GradientFC_pipeline = Pipeline([('tfidf', vectorizer),\n",
    "                ('clf', OneVsRestClassifier(GradientBoostingClassifier())),\n",
    "            ])\n",
    "\n",
    "cv_score1 =cross_val_score(LogReg_pipeline, X, y, scoring = 'accuracy',cv = 10)\n",
    "cv_score2 =cross_val_score(SVC_pipeline, X, y, scoring = 'accuracy',cv = 10)\n",
    "cv_score3 =cross_val_score(RandomFC_pipeline, X, y, scoring = 'accuracy',cv = 10)\n",
    "cv_score4 =cross_val_score(kNN_pipeline, X, y, scoring = 'accuracy',cv = 10)\n",
    "cv_score5 =cross_val_score(GradientFC_pipeline, X, y, scoring = 'accuracy',cv = 10)\n",
    "\n",
    "print('Test accuracy based on 10-fold CV for Logistic Regression is      {}'.format(cv_score1.mean()))\n",
    "print('Test accuracy based on 10-fold CV for Support Vector Classifier is {}'.format(cv_score2.mean()))\n",
    "print('Test accuracy based on 10-fold CV for Random Forest Classifier is {}'.format(cv_score3.mean()))\n",
    "print('Test accuracy based on 10-fold CV for kNearstNeibor is            {}'.format(cv_score4.mean()))\n",
    "print('Test accuracy based on 10-fold CV for Gradient Boosted is         {}'.format(cv_score5.mean()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the 10-Fold accuracy, the best model to determine which category an item belongs to is **Suport Vector Classifier**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Apply the models above on 1000 rows of Full Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df=full_data[~(full_data['product_id'].isin(category_df['product_id'].values))].head(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df['input']=test_df[['brand','brand_category','product_full_name','description','details']].fillna('')\\\n",
    ".agg(' '.join, axis=1).str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df['input']= test_df['input'].apply(lem_sentences)\n",
    "test_df['input'] = test_df['input'].apply(cleanHtml)\n",
    "test_df['input']= test_df['input'].apply(cleanPunc)\n",
    "test_df['input'] = test_df['input'].apply(keepAlpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_text=test_df['input']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Model Deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=category_df['input']\n",
    "y=category_df['clothing']\n",
    "\n",
    "SVC_pipeline = Pipeline([('tfidf', vectorizer),\n",
    "                ('clf', LinearSVC()),\n",
    "            ])\n",
    "\n",
    "SVC_pipeline.fit(X, y)\n",
    "prediction = SVC_pipeline.predict(test_text)\n",
    "test_df['clothing']=prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=fit_df['input']\n",
    "y=fit_df['attribute_value']\n",
    "\n",
    "GradientFC_pipeline = Pipeline([('tfidf', vectorizer),\n",
    "                ('clf', OneVsRestClassifier(GradientBoostingClassifier())),\n",
    "            ])\n",
    "\n",
    "GradientFC_pipeline.fit(X, y)\n",
    "prediction = GradientFC_pipeline.predict(test_text)\n",
    "test_df['fit']=prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=dryclean_df['input']\n",
    "y=dryclean_df['dry_clean']\n",
    "\n",
    "GradientFC_pipeline = Pipeline([('tfidf', vectorizer),\n",
    "                ('clf', GradientBoostingClassifier()),\n",
    "            ])\n",
    "\n",
    "GradientFC_pipeline.fit(X, y)\n",
    "prediction = GradientFC_pipeline.predict(test_text)\n",
    "test_df['dry_clean_only']=prediction\n",
    "test_df['dry_clean_only']=np.where(test_df['dry_clean_only']==1,'yes',\"no\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=category_all2['input']\n",
    "y=category_all2['attribute_category']\n",
    "\n",
    "SVC_pipeline = Pipeline([('tfidf', vectorizer),\n",
    "                ('clf', OneVsRestClassifier(LinearSVC())),\n",
    "            ])\n",
    "\n",
    "SVC_pipeline.fit(X, y)\n",
    "prediction = SVC_pipeline.predict(test_text)\n",
    "test_df['category']=prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df['fit']=np.where(test_df['clothing']==1,test_df['fit'],'')\n",
    "test_df['dry_clean_only']=np.where(test_df['clothing']==1,test_df['dry_clean_only'],'')     \n",
    "test_df['category']=np.where(test_df['category']!='other',test_df['category'],'') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 Sample Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "relaxed             564\n",
       "                    171\n",
       "semi-fitted         149\n",
       "fitted/tailored      72\n",
       "oversized            25\n",
       "straight/regular     19\n",
       "Name: fit, dtype: int64"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df['fit'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "no     537\n",
       "yes    292\n",
       "       171\n",
       "Name: dry_clean_only, dtype: int64"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df['dry_clean_only'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "top          354\n",
       "handbag      174\n",
       "bottom       160\n",
       "             111\n",
       "one piece     79\n",
       "shoe          75\n",
       "scarf         47\n",
       "Name: category, dtype: int64"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df['category'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "#the overall methods are largely based on this work\n",
    "#https://github.com/nkartik94/Multi-Label-Text-Classification/blob/master/Mark_6.ipynb\n",
    "#https://towardsdatascience.com/journey-to-the-center-of-multi-label-classification-384c40229bff"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
